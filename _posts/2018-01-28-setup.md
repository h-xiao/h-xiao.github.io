---
title: "Quantitative Trading Project"
date: 2019-09-01
#tags: [Python, SQL, data gathering, data cleaning, data storing]
header:
  image: "/images/stock.jpg"
mathjax: "true"
---

# Part 1: Getting Financial Data and Storing Data in MySQL


Note: this part assumes you have MySQL, Python, Anaconda installed. 

We will use the mysql.connector to connect to MySQL via Python. If you don't have this package installed, you will need to run this in command line:

```cmd
pip install mysql-connector-python
```


After that is installed, we can import in the package 

```python
import mysql.connector
```


And use this function to create our SQL database (called securities_db).

Note: we will have to set a hostname, username, and password which I've specified in a separate config. 

```python
def CreateDB:

    # Connect to the MySQL instance
    con = mysql.connector.connect(
        host = db_config['host'],
        user = db_config['user'],
        password = db_config['password'] )

    cur = con.cursor(buffered=True)
    cur.execute('CREATE DATABASE securities_db')
    con.commit()

    cur.execute('SHOW DATABASES')
    for db in cur:
        print (db)
```


After the database is set up, we will need to create tables to store the data. Since we don't know the format of the data that we will download yet, it might be good to set up these tables after.

Sample code to create a table:

```python
def CreateTB:

    # Connect to the MySQL instance
    con = mysql.connector.connect(
        host = db_config['host'],
        user = db_config['user'],
        password = db_config['password'],
        database = db_config['database'])

    cur = con.cursor(buffered=True)
    cur.execute('''CREATE TABLE tickers (id int NOT NULL AUTO_INCREMENT, 
    									ticker varchar(32) NOT NULL,
    									PRIMARY KEY (id)) ''')

    con.commit()

    cur.execute('SHOW TABLES')
    for tb in cur:
        print (tb)
```



# Part 2: Getting historical daily pricing data
Let's start off by getting historic data for tickers in S&P 500. We will first need the list of ticker symbols which we can get Wikipedia.

```python
import pandas as pd

data = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')
table = data[0]
```

This can be the first table that we store in MySQL. We will need to first create the table in MySQL:

```python
con = mysql.connector.connect(
        host = db_config['host'],
        user = db_config['user'],
        password = db_config['password'])
cur = con.cursor(buffered=True)

cur.execute('''CREATE TABLE sp_ticker_data (id int NOT NULL AUTO_INCREMENT, 
                                            symbol varchar(32) NOT NULL,
                                            security varchar(255) NOT NULL,
                                            sec_filings varchar(64) NULL,
                                            gics_sector varchar(255) NULL,
                                            gics_sub_industry varchar(255) NULL,
                                            headquarters_location varchar(255) NULL,
                                            date_first_added datetime NULL, 
                                            cik int NULL,
                                            founded int NULL,
                                            PRIMARY KEY (id))''')
```


Then we can store it:

```python
from sqlalchemy import create_engine

engine = create_engine('mysql://' + db_config['user'] + ':' + db_config['password'] + '@' + db_config['host'] + '/' + db_config['database'])
con = engine.connect()
table.to_sql(name='sp_ticker_data', con=con, if_exists='replace')
con.close()
```

We can use this function to query and return all the tables from the database in a dictionary of dataframes.

```
def get_tables_from_db(select_all, tbl_list):
    engine = create_engine('mysql://' + db_config['user'] + ':' + db_config['password'] + '@' + db_config['host'] + '/' + db_config['database'])
    con = engine.connect()

    tables_df = pd.read_sql('SHOW TABLES', con)
    frames_dict = {}
    if select_all == True:
        table_name_list = tables_df.Tables_in_securities_db
    else:
        table_name_list = tbl_list

    select_template = 'SELECT * FROM {table_name}'
    for tname in table_name_list:
        print ('getting table from db: ' + tname)
        query = select_template.format(table_name=tname)
        frames_dict[tname] = pd.read_sql(query, con)

    return frames_dict


frames_dict = get_tables_from_db(True, [])
```

Here is what the first table (sp_ticker_data) that we stored in the database looks like:

[![](/assets/images/setup/sp_ticker_data.JPG)](/assets/images/setup/sp_ticker_data.JPG)


## Part 2a: Getting pricing data from Yahoo Finance

We will use the new Yahoo Finance package to get historic data. 

```python
import yfinance as yf

# where ticker is a string like 'AAPL'
def get_daily_historic_data_yf(ticker):
    data = yf.Ticker(ticker).history(period="max")
    return data
```

We can use the above to function to create a giant df containing all the tickers in S&P and adding an additional column called symbol_id to identify which tickers they are:

[![](/assets/images/setup/daily_data_yf.JPG)](/assets/images/setup/daily_data_yf.JPG)

Then we can create another table in the database called 'daily_data_yf' to store this df using the same method. 

Beside just stock data, we can also get data for other asset classes like FX, commodities, equity indexes, volatility and bitcoin. Most of this data can be obtained using the same yahoo finance package with the exception of commodities which only had historic data for the most recent commodity future (limited to 3 months of daily data). 


## Part 2b: Getting pricing data from Quandl

Quandl provides historic data for the continuous rolled commodities futures. We need to install to this package to access Quandl's API:

```cmd
pip install quandl
```

It's recommended to register for Quandl to get an API key which will increase the amount of daily calls you can make from 50 (unregistered user) to 50,000 (free registered user). 

After that is installed, we can provide a ticker to this function to get historic data from Quandl:

```python
import quandl

quandl.ApiConfig.api_key = api_keys['quandl']   # add your API key here if you have it

def get_daily_historic_data_ql(ticker):
    data = quandl.get(ticker)
    return data
```

Since the columns from Quandl is slightly different from Yahoo Finance, we will create a separate df containing all the tickers we would like to get from Quandl:

[![](/assets/images/setup/daily_data_ql.JPG)](/assets/images/setup/daily_data_ql.JPG)


Then we will store this into our database too, calling it 'daily_data_ql'. 


# Part 3: Getting fundamental data from Yahoo Finance

There's a different Yahoo Finance package that allows us to get some fundamental data. 

```cmd
pip install yahoofinancials
```




```python
from yahoofinancials import YahooFinancials

def get_fundamental_data(sp_ticker_list, is_list, cf_list, bs_list, 
index_dict, tickers_dict):

    errors_df = pd.DataFrame(columns = ['ticker'])
    k = 0

    df_template = pd.DataFrame(columns = is_list + cf_list + bs_list)
    df_out = pd.DataFrame(columns = ['date', 'symbol_id'] + is_list + cf_list + bs_list)

    for i in range(len(sp_ticker_list)):
        print('getting fundamental data for: ' + str(i))
        try:
            ticker = sp_ticker_list[i]
            yf = YahooFinancials(ticker)

            all_statement_data_qt = yf.get_financial_stmts(index_dict['type'], 
                                        ['income', 'cash', 'balance'])

            user_dict = all_statement_data_qt.get(index_dict['is']).get(ticker)
            df1 = pd.DataFrame.from_dict(ChainMap(*user_dict), orient='index')
            df1.columns = [s + '_IS' for s in df1.columns]

            user_dict = all_statement_data_qt.get(index_dict['cf']).get(ticker)
            df2 = pd.DataFrame.from_dict(ChainMap(*user_dict), orient='index')
            df2.columns = [s + '_CF' for s in df2.columns]

            user_dict = all_statement_data_qt.get(index_dict['bs']).get(ticker)
            df3 = pd.DataFrame.from_dict(ChainMap(*user_dict), orient='index')
            df3.columns = [s + '_BS' for s in df3.columns]

            df = pd.concat([df1, df2, df3], axis=1, sort=False).reset_index()
            df = df.rename(columns={'index': 'date'})

            # merge to insert the additional columns that are missing
            df = pd.merge(df, df_template, how="outer")  
            df['symbol_id'] = tickers_dict[ticker]
            col = ['date', 'symbol_id'] + df_template.columns.tolist()
            df = df[col]  # order the columns to match the db that it's going into
            df = df.fillna(value=pd.np.nan)

            df_out = df_out.append(df, ignore_index=True)

        except:
            k = k + 1
            errors_df.loc[k] = [sp_ticker_list[i]]

    # save down errors to look into
    errors_df.to_csv(os.path.join(paths['csv'], 
                    'get_fundamental_data_errors.csv'), index=False)

    return df_out, errors_df
```



























## Setting up MySQL connection in Python





# H1 Heading

## H2 Heading

### H3 Heading

Here's some basic text.

And here's some *italics*

Here's some **bold** text.

What about a [link](https://github.com/dataoptimal)?

Here's a bulleted list:
* First item
+ Second item
- Third item

Here's a numbered list:
1. First
2. Second
3. Third

Python code block:
```python
    import numpy as np

    def test_function(x, y):
      z = np.sum(x,y)
      return z
```

R code block:
```r
library(tidyverse)
df <- read_csv("some_file.csv")
head(df)
```

Here's some inline code `x+y`.

Here's an image:
<img src="{{ site.url }}{{ site.baseurl }}/images/perceptron/linsep.jpg" alt="linearly separable data">

Here's another image using Kramdown:
![alt]({{ site.url }}{{ site.baseurl }}/images/perceptron/linsep.jpg)

Here's some math:

$$z=x+y$$

You can also put it inline $$z=x+y$$
